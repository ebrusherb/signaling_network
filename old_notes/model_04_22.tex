\documentclass{article}
%\documentclass{book}
%\documentstyle[amssymb,amsmath]{article}
%\documentclass[aps,tightenlines,amscd,amsmath,amssymb,verbatim,12pt]{revtex4}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage{custom2}
\usepackage{graphicx} % for figures
\usepackage{epstopdf} % so can use EPS or PDF figures
\usepackage{subfig}
\usepackage{url}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\captionsetup{justification=RaggedRight, singlelinecheck=false}
%  \newcommand{backslash the  command}{what it should do}

\addtolength{\evensidemargin}{-.5in}
\addtolength{\oddsidemargin}{-.5in}
\addtolength{\textwidth}{1.4in}
\addtolength{\textheight}{1.4in}
\addtolength{\topmargin}{-.5in}

\pagestyle{empty}

\begin{document}
\begin{center}
\Large

\end{center}


\vspace{0pt}

\begin{center}
{\bf Development of Signaling Network}
\\ Eleanor Brush
\\ April 22, 2012
\end{center}

\vspace{0pt}
\normalsize
\section{Model: SDE framework}
There are three processes that can cause an animal's estimate of its dominance with respect to another to change:
\begin{enumerate}
\item an error can be made so that the animal's estimate randomly changes from one point in time to the next without an external stimulus

\item external observations provide evidence about each animal's dominance, e.g. by fighting each other they gather evidence about their relative strength

\item receiving a signal provides evidence that the receiver is dominant to the signaler

\end{enumerate}
{\bf Assumption:} For now, we will assume that the two animals have access to the same external observations, which would be the case, for instance, if they only gather evidence from fights that they both are engaged in.  We can relax this assumption in the future.

Let $X_t^{(i)}$ denote animal $i$'s estimate of its dominance at time $t$.  We can therefore right down equations describing how these estimates change over time:
\begin{align*}
X_{t+\tau}^{(1)}=X_t^{(1)}+\sum_{i=0}^{N_{e_1}(\tau)}E_i^{(1)}+\sum_{i=0}^{N_f(\tau)}F_i+b_{s_2}S_2(X_t^{(2)},\tau)
\\ X_{t+\tau}^{(2)}=X_t^{(2)}+\sum_{i=0}^{N_{e_2}(\tau)}E_i^{(2)}-\sum_{i=0}^{N_f(\tau)}F_i+b_{s_1}S_1(X_t^{(1)},\tau)
\end{align*} 
where
\begin{itemize}
\item $E_i^{(j)}$ describes the magnitude of an error in estimate, which are identically distributed over time; $N_{e_j}(\tau)$ describes the number of error events in an interval of length $\tau$; and therefore $\sum_{i=0}^{N_{e_j}(\tau)}E_i^{(j)}$ gives the total magnitude of errors in an interval of length $\tau$

\item $F_i$ describes the magnitude of external evidence, which are identically distributed over time; $N_f(\tau)$ describes the number of external observations in an interval of length $\tau$; and therefore $\sum_{i=0}^{N_f(\tau)}F_i$ gives the total magnitude of external evidence in an interval of length $\tau$ (and the evidence gathered changes the animal's estimate in opposite directions, i.e. if a fight is won, one animal's estimate is increased while the other's is decreased by the same amount)

\item $S_j(X_t^{(j)},\tau)$ gives the number of signals emitted by individual $j$ in an interval of length $\tau$ and $b_{s_j}$ describes the boost that each of those signals would give to animal $j+1'$s estimate of its dominance

\end{itemize}
The fact that the two animals are privy to the same external observations is captured by the identity of the second sums in the above equations.  {\bf Assumption:} The above equations assume that $\tau$ is small enough so that $X_t$ does not change significantly enough in the interval $[t,t+\tau)$ that signaling rates would change.  (Error and fighting rates do not depend on the estimates, another assumption.)


It may seem strange that evidence from errors and fights are summed over several events in the interval $[t,t+\tau)$ while the evidence from signals is just a number of events times a scaling constant.  We could write the evidence from signals as a sum over time, but this would be equivalent to our formulation above since we're assuming the strength of a boost from receiving a signal is constant over time.  

Now we can give the distributions from which these events are drawn:
\begin{itemize}
\item $N_{e_j}(\tau)\sim \scr{P}(r_{e_j},\tau)$
\item $E_i^{(j)}\sim \scr{N}(\mu_{e_j},\sigma^2_{e_j}) \text{ , i.i.d.}$
\item $N_f(\tau)\sim\scr{P}(r_f,\tau)$
\item $F_i\sim\scr{N}(\mu_f,\sigma^2_f)$
\item $S_j(X_t^{(j)},\tau)\sim\scr{P}(f(X_t^{(j)}),\tau)$
\end{itemize}
where $\scr{P}(\lambda,\tau)$ denotes a Poisson process with rate $\lambda$ and $\scr{N}(\mu,\sigma^2)$ denotes a Normal distribution with mean $\mu$ and $\sigma^2$.  $f(x)$ gives the rate (probability) of emitting a signal as a function of the estimate $x$, which is decreasing in $x$.

We now make two approximations:
\begin{enumerate}
\item If $Y_i$, i.i.d.  are drawn from some distribution and $N(\tau)\sim\scr{P}(\lambda,\tau)$, then $Z(\tau)=\sum_{i=0}^{N(\tau)}Y_i$ is a compound Poisson process with 
\begin{align*}
\E[Z(\tau)]&=\E[N(\tau)]\E[Y]=\lambda\tau\E[Y]
\\ \text{ and } Var(Z(\tau))&=\E[N(\tau)]\E[Y^2]=\lambda\tau\E[Y^2]
\end{align*}
For computational convenience, we will approximate the distribution of $Z(\tau)$ by $\scr{N}(\lambda\tau\E[Y],\lambda\tau\E[Y^2])$. (HOW VALID IS THIS?)

\item If $\tau$ is big enough that enough events happen, then we can approximate the Poisson process $\scr{P}(\lambda,\tau)$ with $\scr{N}(\lambda\tau,\lambda\tau)$.

\end{enumerate}

This allows us to rewrite our equations for $X_t$ as:

\begin{align*}
X_{t+\tau}^{(1)}&=X_t^{(1)}+Y_{e_1}(\tau)+Y_f(\tau)+b_{s_2}Y_{s_2}(X_t^{(2)}\tau)
\\ X_{t+\tau}^{(2)}&=X_t^{(2)}+Y_{e_2}(\tau)-Y_f(\tau)+b_{s_1}Y_{s_1}(X_t^{(1)}\tau)
\end{align*}
where 
\begin{itemize}
\item $Y_{e_j}(\tau)\sim\scr{N}(r_{e_j}\tau\mu_{e_j},r_{e_j}\tau(\sigma_{e_j}^2+\mu_{e_j}^2))$

\item $Y_f(\tau)\sim N(r_f\tau\mu_f,r_f\tau(\sigma^2_f+\mu_f^2)$

\item $Y_{s_j}(\tau)\sim\scr{N}(f(X_t^{(j+1)})\tau,f(X_t^{(j+1)})\tau)$

\end{itemize}
Finally, if we define the following constants:
\begin{align*}
m_{e_j}&=r_{e_j}\mu_{e_j},
\\ n_{e_j}^2&=r_{e_j}(\sigma_{e_j}^2+\mu_{e_j}^2),
\\ m_f&=r_f\mu_f,
\\ n_f^2&=r_f(\sigma_f^2+\mu_f^2),
\end{align*}
then we get the following equations:
\begin{align*}
X_{t+\tau}^{(1)}&=X_t^{(1)}+\left[m_{e_1}+m_f+b_{s_2}f(X_t^{(2)})\right]\tau+n_f\sqrt{\tau}Z^{(1)}+n_{e_1}\sqrt{\tau}Z^{(2)}+b_{s_2}f(X_t^{(2)})\sqrt{\tau}Z^{(3)}
\\ dX_t^{(2)}&=X_t^{(2)}+\left[m_{e_2}-m_f+b_{s_1}f(X_t^{(1)})\right]dt+n_f\sqrt{\tau}Z^{(1)}+n_{e_2}\sqrt{\tau}Z^{(4)}+b_{s_1}f(X_t^{(1)})\sqrt{\tau}Z^{(5)}
\end{align*}
where $Z^{(1)},\dots,Z^{(5)}\sim \scr{N}(0,1),$ i.i.d., which are equivalent to the stochastic differential equations:
\begin{align*}
dX_t^{(1)}&=\left[m_{e_1}+m_f+b_{s_2}f(X_t^{(2)})\right]dt+n_fdW_t^{(1)}+n_{e_1}dW_t^{(2)}+b_{s_2}\sqrt{f(X_t^{(2)})}dW_t^{(3)}
\\ dX_t^{(2)}&=\left[m_{e_2}-m_f+b_{s_1}f(X_t^{(1)})\right]dt+n_fdW_t^{(1)}+n_{e_2}dW_t^{(4)}+b_{s_1}\sqrt{f(X_t^{(1)})}dW_t^{(5)}
\end{align*}
where $W_t^{(1)},\dots,dW_t^{(5)}$ are independent Brownian motions.

We could make a few simplifying assumptions to reduce the number of parameters.  {\bf Assumptions:}
\begin{itemize}
\item $r_{e_1}=r_{e_2}$
\item $\mu_{e_1}=\mu_{e_2}$, and even simpler would be to have both equal $0$ (expected error is $0$)
\item $\sigma^2_{e_1}=\sigma^2_{e_2}$
\item $b_{s_1}=b_{s_2}$ 
\item and combining the above would give $m_{e_1}=m_{e_2}$ and $n_{e_1}=n_{e_2}$
\end{itemize}


More generally, consider a system
$$ dX_t=a(X_t)dt+\sum_{i=1}^5\sigma_i(X_t)dW_t^{(i)}$$
with $W_t^{(1)},\dots,W_t^{(5)}$ independent Brownian motions.
If $a,\sigma_1,\dots,\sigma_5:\R^2\to\R^2,$ are ``nice" enough functions, and the random variable $X_t$ has a ``nice" enough density function (i.e. $\E[g(X_t)]=\int g(x)p(t,x)dx$ for every bounded measurable function $g$), then according to the Fokker-Planck / Kolmogorov forward equation
\begin{align*}
\frac{\partial p}{\partial t}&=-\sum_{i=1}^2\frac{\partial}{\partial x_i}(a^{(i)}(x)p(t,x))+\frac{1}{2}\sum_{i,j=1}^2\sum_{k=1}^5\frac{\partial^2}{\partial x_i\partial x_j}(\sigma_k^{(i)}\sigma_k^{(j)}p(t,x)
\\ \Rightarrow \frac{\partial p}{\partial t}&=-(m_{e_1}+m_f+b_{s_2}f(x_2))\frac{\partial p}{\partial x_1}-(m_{e_2}-m_f+b_{s_1}f(x_1))\frac{\partial p}{\partial x_2}
\\&+\frac{1}{2}(n_{e_1}^2+n_f^2+b_{s_2}^2f(x_2))\frac{\partial^2 p}{\partial x_1^2}+n_f^2\frac{\partial p}{\partial x_1\partial x_2}+\frac{1}{2}(n_{e_2}^2+n_f^2+b_{s_1}^2f(x_1))\frac{\partial^2 p}{\partial x_2^2}
\end{align*}
using the functions $a$ and $\sigma_i$ as for our particular model.  Note that this requires the function $f:\R\to\R$ be $C^2$.
\end{document}


